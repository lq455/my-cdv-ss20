# Week 1 Reading Response (Lishan Qin)
***How do technical tools promise to "fair out" the remaining discrimination that exist in social/welfare systems? In how far can they succeed, in which ways do they fail?***

Technical tools fair out discrimination in system by first setting up rules and standards and then analyzing the data collected by the system according to these rules and standards to see if each individual’s data meet the acceptable line. Because both the rules and standards are unchangeable and same to everyone, it seemingly creates an unbiased and equal judging system. However, the collection and calculation of mere number during the process of data analysis wouldn’t take into consideration each individual’s personal issues, for technical tools simply analyze data, while ignoring how the data came into being and reasons behind the collected data. For instance, the data may show that the woman missed the conference and therefore the technical tools may judge the woman as non-compliant, but in reality, the woman simply couldn’t attend the meeting due to physical condition. The judgement given by technical tools may seem unbiased, but that doesn’t mean it’s fair or thorough.

***Imagine, what could this (following quotes) mean in the widest sense?
"The state doesn't need a cop to kill a person" and "electronic incarceration"***

Personal data can be used as a way of punishment by the states. If the social welfare and social status of a person are closely connected to his/her personal data, and the data is collected, interpreted, and controlled by the states, it may easily arrive at the situation where certain corrupted forces in the states can use it as a means to intimidate, threaten and even surpress individual’s free speech and action. States no longer need a policeman to kill people, nor a physical jail to lock and punish prisons, they can simply use these data to deny people housing, health care, and other human rights. What’s more frightening is that their process and even standards of interpreting or analyzing the data is sometimes obscure to citizens.
***What do you understand this to mean?
"systems act as a kind of 'empathy-overwrite'"***

The system supported by the technical tools gets to decide who deserves empathy and who doesn’t. Because these technical tools are made under the assumption that the system only has enough resource to support few people, they try to work out a way to make sure that “the most deserving people got the help” by putting people asking for help under moral diagnosis. But the reality is that it’s likely that all people asking for help need help, and the technical tools ruled them out because they are programmed to limited the number of the people who’s going to get help. The systems decide who should get empathy and who deserves to be poor because they didn’t choose other alternative, while the reality is that they can’t.

***China is much more advanced and expansive when it comes to applying technical solutions to societal processes or instant challenges (recent example). Try to point example cases in China that are in accordance or in opposition to the problematics discussed in the podcast. Perhaps you can think of"technical systems not well thought-through about what their impact on human beings is"***
Applying technical solutions could be extremely helpful in certain cases, such as during the Coronavirus fight, the Chinese government has been using big data to trace the potential virus-carrier who have been in the same bus, train, or plane with people who have been diagnosed as patients. However, as is shown in the New York Times article linked above, such a use of technology could also lead to discrimination. In addition, it is also reported that the technical system could been abused to dox people online. While in some cases it is the government using this technology to track criminals, there are also other cases that this technology has been used by individuals who have access to the system to dox people for their personal interest.                          
